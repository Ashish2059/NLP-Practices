{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPAbrHvrOTFVggNMFxh8fR4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from bs4 import BeautifulSoup\n","import requests\n","import os"],"metadata":{"id":"eqLQ-9UMDsLv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Save all table cpntent  from https://www.nrb.org.np/bank-list/ for evry bank(dropdown value)\n","import csv\n","\n","url = \"https://www.nrb.org.np/bank-list/\"\n","for i in range(17028, 17029):\n","    payload = \"bank=\" + str(i)\n","    headers = {\n","        \"Content-Type\": \"application/x-www-form-urlencoded\",\n","    }\n","    response = requests.post(url, data=payload, headers=headers)\n","    soup = BeautifulSoup(response.content, 'html.parser')\n","    title_div = soup.find('div', {'class': 'card-title'})\n","    title = title_div.text.strip()\n","    table = soup.find('table')\n","    rows = table.find_all('tr')\n","    banks = []\n","    for row in rows:\n","        cells = [cell.text.strip() for cell in row.find_all('td')]\n","        if cells:  \n","            sn = cells[0]\n","            code = cells[1]\n","            address = cells[2]\n","            district = cells[3]\n","            branch_name = cells[4]\n","            open_date = cells[5]\n","            banks.append({\n","                \"S.N.\": sn,\n","                \"Code\": code,\n","                \"Address\": address,\n","                \"District\": district,\n","                \"Branch Name\": branch_name,\n","                \"Open Date\": open_date,\n","            })\n","\n","    filename = title + \".csv\"\n","    with open(filename, 'w', newline='') as file:\n","        writer = csv.DictWriter(file, fieldnames=banks[0].keys())\n","        writer.writeheader()\n","        writer.writerows(banks)"],"metadata":{"id":"z61T4xSdTt9X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" #Downloading all the pdf from the link https://www.nrb.org.np/category/monthly-statistics/ ,https://www.nrb.org.np/category/monthly-statistics/page/2 , https://www.nrb.org.np/category/monthly-statistics/page/3 and so on\n","import os\n","\n","def extract_url_pdf(input_url,folder_path=os.getcwd()):\n","    \n","    import os\n","    import requests\n","    from urllib.parse import urljoin\n","    from bs4 import BeautifulSoup\n","    import pandas as pd\n","    import datetime\n","    \n","    url = input_url\n","\n","    #If there is no such folder, the script will create one automatically\n","    folder_location = folder_path\n","    if not os.path.exists(folder_location):os.mkdir(folder_location)\n","\n","    response = requests.get(url)\n","    soup= BeautifulSoup(response.text, \"html.parser\") \n","\n","    link_text=list()\n","    link_href=list()\n","    link_file=list()\n","    \n","    counter=0\n","\n","    for link in soup.select(\"a[href$='.pdf']\"):\n","        #Name the pdf files using the last portion of each link which are unique in this case\n","        \n","        filename = os.path.join(folder_location,link['href'].split('/')[-1])\n","        with open(filename, 'wb') as f:\n","            f.write(requests.get(urljoin(url,link['href'])).content)\n","            \n","        link_text.append(str(link.text))\n","        \n","        link_href.append(link['href'])\n","\n","        link_file.append(link['href'].split('/')[-1])\n","        \n","        counter+=1\n","\n","        print(counter, \"-Files Extracted from URL named \",link['href'].split('/')[-1])\n","        \n","    table_dict={\"Text\":link_text,\"Url_Link\":link_href,\"File Name\":link_file}\n","\n","    df=pd.DataFrame(table_dict)\n","    \n","    time_stamp = datetime.datetime.now().strftime('%Y-%m-%d %H-%M-%S')\n","\n","    print(\"All Pdf files downloaded \\n\")\n","\n","\n","\n","link=[]\n","extract_url_pdf(input_url=\"https://www.nrb.org.np/category/monthly-statistics/\")\n","url=\"https://www.nrb.org.np/category/monthly-statistics/\"\n","for i in range(2,16):\n","  i=str(i)\n","  pages=\"page/\"+i+\"/\"\n","  input_url=url+pages\n","  print(input_url)\n","  extract_url_pdf(input_url)\n","    \n","\n"," "],"metadata":{"id":"LJiC1JDgIuth"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get all the url from https://www.nrb.org.np/category/monthly-statistics/ ,https://www.nrb.org.np/category/monthly-statistics/page/2 , https://www.nrb.org.np/category/monthly-statistics/page/3 \n","# and so on that doesnot end with .xlsx\n","\n","\n","def get_url(input_url):\n","    url = input_url\n","    response = requests.get(url)\n","    soup= BeautifulSoup(response.text, \"html.parser\") \n","    links = []\n","    ul = soup.find(\"ul\",{\"class\": \"arrowed-list arrowed-list--border\"})  # Find the unordered list containing the links\n","    if ul:\n","      for link in ul.find_all(\"a\"):\n","        link_url = link.get(\"href\")\n","        if link_url is not None:\n","            if link_url is not None and not link_url.endswith(\".xlsx\"):  # Exclude links with the specified URL pattern\n","              links.append(link_url)\n","    print(\"\\n\")\n","    for link in links:\n","        print(link)\n","      \n","\n","\n","input_url=\"https://www.nrb.org.np/category/monthly-statistics/\"\n","get_url(input_url)\n","url=\"https://www.nrb.org.np/category/monthly-statistics/\"\n","for i in range(2,16):\n","  i=str(i)\n","  pages=\"page/\"+i+\"/\"\n","  input_url=url+pages\n","  print(\"\\n\")\n","  get_url(input_url)"],"metadata":{"id":"y-oHQkv7OTZf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Download pdf from different urls\n","\n","url = [\"https://www.nrb.org.np/contents/uploads/2023/03/Magh_2079_Publish.pdf\",\n","         \"https://www.nrb.org.np/contents/uploads/2019/12/Monthly_Statistics-2070-01-Mid-May2013.pdf\",\n","         \"https://www.nrb.org.np/contents/uploads/2019/12/Asar_2070.pdf\"]\n","\n","folder_location = os.getcwd()\n","\n","if not os.path.exists(folder_location):\n","    os.mkdir(folder_location)\n","\n","for url in url:\n","    response = requests.get(url)\n","\n","    if response.status_code == 200:\n","       file_name = url.split(\"/\")[-1]\n","       response = requests.get(url)\n","       with open(os.path.join(folder_location, file_name), \"wb\") as f:\n","          f.write(response.content)\n","\n"],"metadata":{"id":"jwEU1BzRWDPE"},"execution_count":null,"outputs":[]}]}